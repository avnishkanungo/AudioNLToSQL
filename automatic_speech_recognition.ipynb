{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1786111935455bac3748f14bbc63e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "        num_rows: 563\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "poly_ai = load_dataset(\"PolyAI/minds14\", \"en-US\")\n",
    "\n",
    "poly_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_use = DatasetDict()\n",
    "\n",
    "dataset_to_use[\"train\"] = Dataset.from_dict(poly_ai[\"train\"][0:450])\n",
    "dataset_to_use[\"test\"] = Dataset.from_dict(poly_ai[\"train\"][450:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "        num_rows: 450\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "        num_rows: 113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array': [0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.00048828125,\n",
       "  -0.00048828125,\n",
       "  0.000732421875,\n",
       "  -0.0009765625,\n",
       "  0.001220703125,\n",
       "  -0.003662109375,\n",
       "  0.0050048828125,\n",
       "  0.0074462890625,\n",
       "  -0.00146484375,\n",
       "  0.001220703125,\n",
       "  -0.001220703125,\n",
       "  0.000244140625,\n",
       "  -0.0009765625,\n",
       "  -0.000244140625,\n",
       "  -0.001220703125,\n",
       "  -0.000732421875,\n",
       "  -0.000732421875,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  -0.0009765625,\n",
       "  -0.000732421875,\n",
       "  -0.001220703125,\n",
       "  -0.000732421875,\n",
       "  -0.000732421875,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000732421875,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000732421875,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.0,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000732421875,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  0.00048828125,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.00048828125,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  -0.000244140625,\n",
       "  -0.000244140625,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  0.000244140625,\n",
       "  ...],\n",
       " 'path': '/home/zeus/.cache/huggingface/datasets/downloads/extracted/8c61860ed1376958d9fb973ebab155bcdd78efa149bc5217aa38a302b0d850dc/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n",
       " 'sampling_rate': 8000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_use[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"english\", task=\"transcribe\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "dataset_to_use = dataset_to_use.cast_column(\"audio\",Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"english_transcription\"],\n",
    "    )\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': array([[[-0.67151165, -0.62832355, -0.67151165, ..., -0.67151165,\n",
       "         -0.67151165, -0.67151165],\n",
       "        [-0.67151165, -0.6334348 , -0.67151165, ..., -0.67151165,\n",
       "         -0.67151165, -0.67151165],\n",
       "        [-0.67151165, -0.67151165, -0.67151165, ..., -0.67151165,\n",
       "         -0.67151165, -0.67151165],\n",
       "        ...,\n",
       "        [-0.67151165, -0.67151165, -0.67151165, ..., -0.67151165,\n",
       "         -0.67151165, -0.67151165],\n",
       "        [-0.67151165, -0.67151165, -0.67151165, ..., -0.67151165,\n",
       "         -0.67151165, -0.67151165],\n",
       "        [-0.67151165, -0.67151165, -0.67151165, ..., -0.67151165,\n",
       "         -0.67151165, -0.67151165]]], dtype=float32), 'labels': [50258, 50259, 50359, 50363, 40, 576, 411, 281, 992, 493, 257, 7225, 2696, 365, 452, 4975, 50257], 'input_length': 10.837375}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_dataset(dataset_to_use[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f316d674ef4a7cbe20e8a532f62d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9255b18c30c9472c954a93de74441ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_to_use = dataset_to_use.map(prepare_dataset,remove_columns=dataset_to_use.column_names[\"train\"], num_proc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels', 'input_length'],\n",
       "        num_rows: 450\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels', 'input_length'],\n",
       "        num_rows: 113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d669c6acc745b4aac0523a5feb5957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "dataset_to_use[\"tain\"] = dataset_to_use[\"train\"].filter(is_audio_in_length_range, input_columns = ['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor = processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jiwer) (3.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic wer\n",
    "    wer_ortho = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"english\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1504: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\",  # name on the HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=4000,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset_to_use[\"train\"],\n",
    "    eval_dataset=dataset_to_use[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 2:42:19, Epoch 137/138]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer Ortho</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.602929</td>\n",
       "      <td>0.254781</td>\n",
       "      <td>0.257969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656808</td>\n",
       "      <td>0.247995</td>\n",
       "      <td>0.253247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690974</td>\n",
       "      <td>0.249846</td>\n",
       "      <td>0.255608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.717311</td>\n",
       "      <td>0.247995</td>\n",
       "      <td>0.253837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740206</td>\n",
       "      <td>0.248612</td>\n",
       "      <td>0.254427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762435</td>\n",
       "      <td>0.256632</td>\n",
       "      <td>0.262102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.779991</td>\n",
       "      <td>0.264035</td>\n",
       "      <td>0.269185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796162</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>0.267414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.03593853246121944, metrics={'train_runtime': 9744.4921, 'train_samples_per_second': 6.568, 'train_steps_per_second': 0.41, 'total_flos': 1.791595882266624e+19, 'train_loss': 0.03593853246121944, 'epoch': 137.93103448275863})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    # \"dataset_tags\": \"PolyAI/minds14\",\n",
    "    \"dataset\":\"minds14\",\n",
    "    \"finetuned_from\": \"openai/whisper-tiny\",\n",
    "    \"model_name\": \"openai/whisper-small-finetuned-gtzan\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/avnishkanungo/whisper-small-dv/commit/d3c277ee647a5de21d314d2e9dc819b71650b687', commit_message='End of training', commit_description='', oid='d3c277ee647a5de21d314d2e9dc819b71650b687', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"avnishkanungo/whisper-small-dv\"  # update with your model id\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath):\n",
    "    output = pipe(\n",
    "        filepath,\n",
    "        max_new_tokens=256,\n",
    "        generate_kwargs={\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"english\",\n",
    "        },  # update with the language you've fine-tuned on\n",
    "        chunk_length_s=30,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the stale smell of old beer lingers it takes heat to bring out the odor a cold dip restores health and zest a salt pickle tastes fine with ham tacos all pastore are my favorite a zestful food is the hot cross bun'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe_speech(\"/teamspace/studios/this_studio/harvard.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Downloading sounddevice-0.4.7-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_speech(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  libportaudio2\n",
      "0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\n",
      "Need to get 65.4 kB of archives.\n",
      "After this operation, 223 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudio2 amd64 19.6.0-1build1 [65.4 kB]\n",
      "Fetched 65.4 kB in 0s (1073 kB/s)        \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libportaudio2:amd64.\n",
      "(Reading database ... 64811 files and directories currently installed.)\n",
      "Preparing to unpack .../libportaudio2_19.6.0-1build1_amd64.deb ...\n",
      "Unpacking libportaudio2:amd64 (19.6.0-1build1) ...\n",
      "Setting up libportaudio2:amd64 (19.6.0-1build1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install libportaudio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libass9 libasyncns0 libavc1394-0 libavdevice58 libavfilter7 libavresample4\n",
      "  libbs2b0 libcaca0 libcdio-cdda2 libcdio-paranoia2 libcdio18 libdc1394-22\n",
      "  libfftw3-double3 libflac8 libflite1 libiec61883-0 libjack-jackd2-0\n",
      "  liblilv-0-0 libmysofa1 libnorm1 libopenal-data libopenal1 libpgm-5.2-0\n",
      "  libpostproc55 libpulse0 libraw1394-11 librubberband2 libsamplerate0\n",
      "  libsdl2-2.0-0 libserd-0-0 libslang2 libsndfile1 libsndio7.0 libsodium23\n",
      "  libsord-0-0 libsratom-0-0 libusb-1.0-0 libvidstab1.1 libxss1 libxv1 libzmq5\n",
      "Suggested packages:\n",
      "  ffmpeg-doc libfftw3-bin libfftw3-dev jackd2 libportaudio2 pulseaudio\n",
      "  libraw1394-doc serdi sndiod sordi\n",
      "The following NEW packages will be installed:\n",
      "  ffmpeg libass9 libasyncns0 libavc1394-0 libavdevice58 libavfilter7\n",
      "  libavresample4 libbs2b0 libcaca0 libcdio-cdda2 libcdio-paranoia2 libcdio18\n",
      "  libdc1394-22 libfftw3-double3 libflac8 libflite1 libiec61883-0\n",
      "  libjack-jackd2-0 liblilv-0-0 libmysofa1 libnorm1 libopenal-data libopenal1\n",
      "  libpgm-5.2-0 libpostproc55 libpulse0 libraw1394-11 librubberband2\n",
      "  libsamplerate0 libsdl2-2.0-0 libserd-0-0 libslang2 libsndfile1 libsndio7.0\n",
      "  libsodium23 libsord-0-0 libsratom-0-0 libusb-1.0-0 libvidstab1.1 libxss1\n",
      "  libxv1 libzmq5\n",
      "0 upgraded, 42 newly installed, 0 to remove and 3 not upgraded.\n",
      "Need to get 21.3 MB of archives.\n",
      "After this operation, 51.4 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libslang2 amd64 2.3.2-4 [429 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libsodium23 amd64 1.0.18-1 [150 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libusb-1.0-0 amd64 2:1.0.23-2build1 [46.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libraw1394-11 amd64 2.1.2-1 [30.7 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libavc1394-0 amd64 0.5.4-5 [16.2 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 libass9 amd64 1:0.14.0-2 [88.0 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libbs2b0 amd64 3.1.0+dfsg-2.2build1 [10.2 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libflite1 amd64 2.1-release-3 [12.8 MB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 libserd-0-0 amd64 0.30.2-1 [46.6 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsord-0-0 amd64 0.16.4-1 [19.5 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsratom-0-0 amd64 0.6.4-1 [16.9 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 liblilv-0-0 amd64 0.24.6-1ubuntu0.1 [40.6 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/universe amd64 libmysofa1 amd64 1.0~dfsg0-1 [39.2 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpostproc55 amd64 7:4.2.7-0ubuntu0.1 [55.0 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libfftw3-double3 amd64 3.3.8-2ubuntu1 [728 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libsamplerate0 amd64 0.1.9-2 [939 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/universe amd64 librubberband2 amd64 1.8.2-1build1 [89.4 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/universe amd64 libvidstab1.1 amd64 1.1.0-2 [35.0 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal/universe amd64 libnorm1 amd64 1.5.8+dfsg2-2build1 [290 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpgm-5.2-0 amd64 5.2.122~dfsg-3ubuntu1 [158 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal/universe amd64 libzmq5 amd64 4.3.2-2ubuntu1 [242 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libavfilter7 amd64 7:4.2.7-0ubuntu0.1 [1085 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcaca0 amd64 0.99.beta19-2.1ubuntu1.20.04.2 [203 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcdio18 amd64 2.0.0-2ubuntu0.2 [59.4 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal/main amd64 libcdio-cdda2 amd64 10.2+2.0.0-1 [17.6 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 libcdio-paranoia2 amd64 10.2+2.0.0-1 [16.2 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu focal/universe amd64 libdc1394-22 amd64 2.2.5-2.1 [79.6 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu focal/main amd64 libiec61883-0 amd64 1.2.0-3 [24.3 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu focal/main amd64 libjack-jackd2-0 amd64 1.9.12~dfsg-2ubuntu2 [267 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopenal-data all 1:1.19.1-1 [162 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsndio7.0 amd64 1.5.0-3 [24.5 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopenal1 amd64 1:1.19.1-1 [492 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu focal/main amd64 libasyncns0 amd64 0.8-6 [12.1 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libflac8 amd64 1.3.3-1ubuntu0.2 [103 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsndfile1 amd64 1.0.28-7ubuntu0.2 [170 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpulse0 amd64 1:13.99.1-1ubuntu3.13 [262 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu focal/main amd64 libxss1 amd64 1:1.2.3-1 [8140 B]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsdl2-2.0-0 amd64 2.0.10+dfsg1-3 [407 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu focal/main amd64 libxv1 amd64 2:1.0.11-1 [10.7 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libavdevice58 amd64 7:4.2.7-0ubuntu0.1 [74.3 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libavresample4 amd64 7:4.2.7-0ubuntu0.1 [54.2 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 ffmpeg amd64 7:4.2.7-0ubuntu0.1 [1453 kB]\n",
      "Fetched 21.3 MB in 1s (17.1 MB/s) \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libslang2:amd64.\n",
      "(Reading database ... 64485 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libslang2_2.3.2-4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libslang2:amd64 (2.3.2-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Selecting previously unselected package libsodium23:amd64.\n",
      "Preparing to unpack .../01-libsodium23_1.0.18-1_amd64.deb ...\n",
      "Unpacking libsodium23:amd64 (1.0.18-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package libusb-1.0-0:amd64.\n",
      "Preparing to unpack .../02-libusb-1.0-0_2%3a1.0.23-2build1_amd64.deb ...\n",
      "Unpacking libusb-1.0-0:amd64 (2:1.0.23-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libraw1394-11:amd64.\n",
      "Preparing to unpack .../03-libraw1394-11_2.1.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libraw1394-11:amd64 (2.1.2-1) ...\n",
      "Selecting previously unselected package libavc1394-0:amd64.\n",
      "Preparing to unpack .../04-libavc1394-0_0.5.4-5_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libavc1394-0:amd64 (0.5.4-5) ...\n",
      "Selecting previously unselected package libass9:amd64.\n",
      "Preparing to unpack .../05-libass9_1%3a0.14.0-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libass9:amd64 (1:0.14.0-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libbs2b0:amd64.\n",
      "Preparing to unpack .../06-libbs2b0_3.1.0+dfsg-2.2build1_amd64.deb ...\n",
      "Unpacking libbs2b0:amd64 (3.1.0+dfsg-2.2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libflite1:amd64.\n",
      "Preparing to unpack .../07-libflite1_2.1-release-3_amd64.deb ...\n",
      "Unpacking libflite1:amd64 (2.1-release-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libserd-0-0:amd64.\n",
      "Preparing to unpack .../08-libserd-0-0_0.30.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libserd-0-0:amd64 (0.30.2-1) ...\n",
      "Selecting previously unselected package libsord-0-0:amd64.\n",
      "Preparing to unpack .../09-libsord-0-0_0.16.4-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libsord-0-0:amd64 (0.16.4-1) ...\n",
      "Selecting previously unselected package libsratom-0-0:amd64.\n",
      "Preparing to unpack .../10-libsratom-0-0_0.6.4-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libsratom-0-0:amd64 (0.6.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package liblilv-0-0:amd64.\n",
      "Preparing to unpack .../11-liblilv-0-0_0.24.6-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking liblilv-0-0:amd64 (0.24.6-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libmysofa1:amd64.\n",
      "Preparing to unpack .../12-libmysofa1_1.0~dfsg0-1_amd64.deb ...\n",
      "Unpacking libmysofa1:amd64 (1.0~dfsg0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libpostproc55:amd64.\n",
      "Preparing to unpack .../13-libpostproc55_7%3a4.2.7-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libpostproc55:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libfftw3-double3:amd64.\n",
      "Preparing to unpack .../14-libfftw3-double3_3.3.8-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libfftw3-double3:amd64 (3.3.8-2ubuntu1) ...\n",
      "Selecting previously unselected package libsamplerate0:amd64.\n",
      "Preparing to unpack .../15-libsamplerate0_0.1.9-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libsamplerate0:amd64 (0.1.9-2) ...\n",
      "Selecting previously unselected package librubberband2:amd64.\n",
      "Preparing to unpack .../16-librubberband2_1.8.2-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking librubberband2:amd64 (1.8.2-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libvidstab1.1:amd64.\n",
      "Preparing to unpack .../17-libvidstab1.1_1.1.0-2_amd64.deb ...\n",
      "Unpacking libvidstab1.1:amd64 (1.1.0-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libnorm1:amd64.\n",
      "Preparing to unpack .../18-libnorm1_1.5.8+dfsg2-2build1_amd64.deb ...\n",
      "Unpacking libnorm1:amd64 (1.5.8+dfsg2-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libpgm-5.2-0:amd64.\n",
      "Preparing to unpack .../19-libpgm-5.2-0_5.2.122~dfsg-3ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libpgm-5.2-0:amd64 (5.2.122~dfsg-3ubuntu1) ...\n",
      "Selecting previously unselected package libzmq5:amd64.\n",
      "Preparing to unpack .../20-libzmq5_4.3.2-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libzmq5:amd64 (4.3.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libavfilter7:amd64.\n",
      "Preparing to unpack .../21-libavfilter7_7%3a4.2.7-0ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libavfilter7:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libcaca0:amd64.\n",
      "Preparing to unpack .../22-libcaca0_0.99.beta19-2.1ubuntu1.20.04.2_amd64.deb ...\n",
      "Unpacking libcaca0:amd64 (0.99.beta19-2.1ubuntu1.20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libcdio18:amd64.\n",
      "Preparing to unpack .../23-libcdio18_2.0.0-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking libcdio18:amd64 (2.0.0-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libcdio-cdda2:amd64.\n",
      "Preparing to unpack .../24-libcdio-cdda2_10.2+2.0.0-1_amd64.deb ...\n",
      "Unpacking libcdio-cdda2:amd64 (10.2+2.0.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libcdio-paranoia2:amd64.\n",
      "Preparing to unpack .../25-libcdio-paranoia2_10.2+2.0.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libcdio-paranoia2:amd64 (10.2+2.0.0-1) ...\n",
      "Selecting previously unselected package libdc1394-22:amd64.\n",
      "Preparing to unpack .../26-libdc1394-22_2.2.5-2.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libdc1394-22:amd64 (2.2.5-2.1) ...\n",
      "Selecting previously unselected package libiec61883-0:amd64.\n",
      "Preparing to unpack .../27-libiec61883-0_1.2.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libiec61883-0:amd64 (1.2.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libjack-jackd2-0:amd64.\n",
      "Preparing to unpack .../28-libjack-jackd2-0_1.9.12~dfsg-2ubuntu2_amd64.deb ...\n",
      "Unpacking libjack-jackd2-0:amd64 (1.9.12~dfsg-2ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libopenal-data.\n",
      "Preparing to unpack .../29-libopenal-data_1%3a1.19.1-1_all.deb ...\n",
      "Unpacking libopenal-data (1:1.19.1-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libsndio7.0:amd64.\n",
      "Preparing to unpack .../30-libsndio7.0_1.5.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Unpacking libsndio7.0:amd64 (1.5.0-3) ...\n",
      "Selecting previously unselected package libopenal1:amd64.\n",
      "Preparing to unpack .../31-libopenal1_1%3a1.19.1-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking libopenal1:amd64 (1:1.19.1-1) ...\n",
      "Selecting previously unselected package libasyncns0:amd64.\n",
      "Preparing to unpack .../32-libasyncns0_0.8-6_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libasyncns0:amd64 (0.8-6) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libflac8:amd64.\n",
      "Preparing to unpack .../33-libflac8_1.3.3-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libflac8:amd64 (1.3.3-1ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libsndfile1:amd64.\n",
      "Preparing to unpack .../34-libsndfile1_1.0.28-7ubuntu0.2_amd64.deb ...\n",
      "Unpacking libsndfile1:amd64 (1.0.28-7ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libpulse0:amd64.\n",
      "Preparing to unpack .../35-libpulse0_1%3a13.99.1-1ubuntu3.13_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libpulse0:amd64 (1:13.99.1-1ubuntu3.13) ...\n",
      "Selecting previously unselected package libxss1:amd64.\n",
      "Preparing to unpack .../36-libxss1_1%3a1.2.3-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libxss1:amd64 (1:1.2.3-1) ...\n",
      "Selecting previously unselected package libsdl2-2.0-0:amd64.\n",
      "Preparing to unpack .../37-libsdl2-2.0-0_2.0.10+dfsg1-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libsdl2-2.0-0:amd64 (2.0.10+dfsg1-3) ...\n",
      "Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../38-libxv1_2%3a1.0.11-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libxv1:amd64 (2:1.0.11-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Selecting previously unselected package libavdevice58:amd64.\n",
      "Preparing to unpack .../39-libavdevice58_7%3a4.2.7-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavdevice58:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package libavresample4:amd64.\n",
      "Preparing to unpack .../40-libavresample4_7%3a4.2.7-0ubuntu0.1_amd64.deb ...\n",
      "Unpacking libavresample4:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Selecting previously unselected package ffmpeg.\n",
      "Preparing to unpack .../41-ffmpeg_7%3a4.2.7-0ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking ffmpeg (7:4.2.7-0ubuntu0.1) ...\n",
      "Setting up libraw1394-11:amd64 (2.1.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libsodium23:amd64 (1.0.18-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libnorm1:amd64 (1.5.8+dfsg2-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libmysofa1:amd64 (1.0~dfsg0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libcdio18:amd64 (2.0.0-2ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libavresample4:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libflac8:amd64 (1.3.3-1ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libass9:amd64 (1:0.14.0-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libslang2:amd64 (2.3.2-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libxv1:amd64 (2:1.0.11-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libpostproc55:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libfftw3-double3:amd64 (3.3.8-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libsndio7.0:amd64 (1.5.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libvidstab1.1:amd64 (1.1.0-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libflite1:amd64 (2.1-release-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libasyncns0:amd64 (0.8-6) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libbs2b0:amd64 (3.1.0+dfsg-2.2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libopenal-data (1:1.19.1-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libxss1:amd64 (1:1.2.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libusb-1.0-0:amd64 (2:1.0.23-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libsndfile1:amd64 (1.0.28-7ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libsamplerate0:amd64 (0.1.9-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libpgm-5.2-0:amd64 (5.2.122~dfsg-3ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libiec61883-0:amd64 (1.2.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [############################################..............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libserd-0-0:amd64 (0.30.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libavc1394-0:amd64 (0.5.4-5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libzmq5:amd64 (4.3.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libcaca0:amd64 (0.99.beta19-2.1ubuntu1.20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libpulse0:amd64 (1:13.99.1-1ubuntu3.13) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libcdio-cdda2:amd64 (10.2+2.0.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libcdio-paranoia2:amd64 (10.2+2.0.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libdc1394-22:amd64 (2.2.5-2.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libopenal1:amd64 (1:1.19.1-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up librubberband2:amd64 (1.8.2-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libjack-jackd2-0:amd64 (1.9.12~dfsg-2ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libsord-0-0:amd64 (0.16.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libsratom-0-0:amd64 (0.6.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libsdl2-2.0-0:amd64 (2.0.10+dfsg1-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up liblilv-0-0:amd64 (0.24.6-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libavfilter7:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libavdevice58:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up ffmpeg (7:4.2.7-0ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.16) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "    outputs=gr.components.Textbox(),\n",
    ")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.components.Textbox(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'http_server' from 'gradio' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m demo:\n\u001b[1;32m      2\u001b[0m     gr\u001b[38;5;241m.\u001b[39mTabbedInterface(\n\u001b[1;32m      3\u001b[0m         [mic_transcribe, file_transcribe],\n\u001b[1;32m      4\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribe Microphone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribe Audio File\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/blocks.py:2320\u001b[0m, in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, _frontend, enable_monitoring)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     if self.share_url is not None:\n\u001b[1;32m   2319\u001b[0m         mlflow.log_param(\"Gradio Interface Share Link\", self.share_url)\n\u001b[0;32m-> 2320\u001b[0m     else:\n\u001b[1;32m   2321\u001b[0m         mlflow.log_param(\"Gradio Interface Local Link\", self.local_url)\n\u001b[1;32m   2322\u001b[0m if self.analytics_enabled and analytics_integration:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'http_server' from 'gradio' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/__init__.py)"
     ]
    }
   ],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
